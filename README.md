# ğŸš€ Customer Segmentation: Datengetriebene Einblicke 

<!-- Eine kurze, prÃ¤gnante Beschreibung deines Data Science Projekts in 1-2 SÃ¤tzen.-->
In diesem Projekt untersuche ich ein E-Commerce-Datenset mit Informationen Ã¼ber Kunden, Bestellungen und Produkte. 


## ğŸ“Š ProjektÃ¼bersicht

**Problemstellung:** 
<!-- Beschreibe das Problem, das du lÃ¶sen mÃ¶chtest -->

FÃ¼r das Ziel mÃ¼ssen die Daten zunÃ¤chst grÃ¼ndlich bereinigt werden, da sie sehr roh und unstrukturiert sind. AnschlieÃŸend kÃ¶nnen wir durch eine explorative Analyse erste Muster, ZusammenhÃ¤nge und wichtige Kennzahlen erkennen, die helfen, das Kunden- und Kaufverhalten besser zu verstehen.

### **Ziel:** 

<!-- Was ist das Hauptziel deines Projekts? -->
Ziel ist es, das Kaufverhalten besser zu verstehen, wichtige Kundengruppen zu erkennen und erste Muster in den Daten sichtbar zu machen. 

**Das Kundenverhalten verstehen und zentrale Segmente identifizieren**

- Ermitteln, welche Kundengruppen den hÃ¶chsten Umsatz generieren.
- Kaufmuster analysieren: HÃ¤ufigkeit, durchschnittlicher Bestellwert, Produkttypen, VertriebskanÃ¤le.
- Welche Produkte sind am profitabelsten?
- Wie beeinflussen Rabatte die Conversion Rate und den durchschnittlichen Warenkorbwert?


**Methoden:** 
<!-- Welche Techniken/Algorithmen verwendest du? -->

âœï¸ spÃ¤ter


## Â Datenquelle

Die Rohdaten stammen aus dem Kaggle-Datensatz Customer Segmentation:  
https://www.kaggle.com/datasets/yunusemretokdemir/customer-segmentation/data

Es handelt sich um unstrukturierte Rohdaten aus einer E-Commerce-Datenbank, vollstÃ¤ndig in einer einzigen Tabelle abgelegt und nicht fÃ¼r Analysen vorbereitet. Die Datei enthÃ¤lt viele leere oder unvollstÃ¤ndige Spalten.

Nach Entfernen vollstÃ¤ndig leerer Spalten wurden **60 Spalten** fÃ¼r die weitere Verarbeitung geladen.  
Die Daten kombinieren Kunden-, Bestell-, Bestellpositions- und Produktinformationen, jedoch ohne klare Trennung der EntitÃ¤ten.

### Warum diese Daten?

Um auch meine FÃ¤higkeiten im Umgang mit â€Daten-Bolzenâ€œ â€” also unaufgerÃ¤umten, realitÃ¤tsnahen Rohdaten â€” zu demonstrieren und diese in eine analysierbare Form zu bringen.



##  Data Cleaning 

Die geladenen Rohdaten waren unstrukturiert und enthielten zahlreiche fehlende, redundante oder technisch erzeugte Spalten. Daher wurden folgende Schritte durchgefÃ¼hrt:

- Entfernen von irrelevanten Spalten sowie von Spalten, in denen kaum oder praktisch keine verwertbaren Daten vorhanden waren
- Umwandlung und Vereinheitlichung der Datentypen  
    (Datumsfelder â†’ datetime, Kategorien â†’ category).
- Trennung des ursprÃ¼nglichen Datensatzes in vier logische EntitÃ¤ten:  
    **Customers**, **Orders**, **Order_Items** und **Products**.
- Behandlung fehlender Werte (gezielte Imputation oder bewusstes Belassen â€“ abhÃ¤ngig von ihrer analytischen Relevanz)
- Bereinigung von Duplikaten anhand sinnvoller SchlÃ¼sselattribute  
    (z. B. Kunden-ID + Erstellungsdatum, Bestell-ID + Bestelldatum).
- Entfernen von DatensÃ¤tzen ohne Produkt-ID sowie ÃœberprÃ¼fung auf logische Konsistenz.
- Feature Engineering: HinzufÃ¼gen der Kennzahl cost_price_ratio zur Bewertung der Marge je Bestellposition.
- **Nach der Bereinigung liegen nun vier getrennte, klar strukturierte und analysierbare DataFrames vor sowie ein zusammengefÃ¼hrter Gesamtdatensatz (55 Spalten, 4194 Zeilen). Diese bilden die Grundlage fÃ¼r die weitere explorative Datenanalyse.**
    
    Der bereinigte Datenbestand umfasst:
    
    - **3054 eindeutige Kunden**
        
    - **3565 Bestellungen**
        
    - **4194 Bestellpositionen (Order Items)**
        
    - **1710 einzigartige Produkte**


## ğŸ“Š Explorative Datenanalyse (EDA)

âœï¸ spÃ¤ter


## ğŸ“ˆ Insights & Ergebnisse

âœï¸ spÃ¤ter

* * *

## Setup

Klone das Repository
```bash
# Repository klonen
git clone [DEIN-REPO-LINK]
cd [REPO-NAME]
```

Installiere [uv](https://uv.dev) (falls noch nicht installiert) und synchronisiere die AbhÃ¤ngigkeiten
```bash
# Dependencies installieren
uv sync
```

### AusfÃ¼hrung 

Notebooks in dieser Reihenfolge ausfÃ¼hren:

1. notebooks/01_exploration.ipynb - raw owerview
2. notebooks/02_preprocessing.ipynb - clean
3. notebooks/03_modeling.ipynb - deep EDA
4. notebooks/04_results.ipynb 
<!--



-->


